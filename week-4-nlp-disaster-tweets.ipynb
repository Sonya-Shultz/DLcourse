{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import all needed libraries\n\n**For math and schemes:**","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-23T17:02:22.121400Z","iopub.execute_input":"2022-05-23T17:02:22.122218Z","iopub.status.idle":"2022-05-23T17:02:22.141489Z","shell.execute_reply.started":"2022-05-23T17:02:22.122138Z","shell.execute_reply":"2022-05-23T17:02:22.140716Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":"**For data porocessing & model creation**","metadata":{}},{"cell_type":"code","source":"import nltk # for words filtration\nfrom nltk.corpus import stopwords\nimport re # for string maniputations\nfrom sklearn.model_selection import train_test_split\n# to process data\nfrom keras.preprocessing.text import Tokenizer \nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\n# for creating models\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding,GRU, LSTM, RNN\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nimport keras.backend as K\n# for beautiful representation\nfrom wordcloud import WordCloud","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:02:22.143455Z","iopub.execute_input":"2022-05-23T17:02:22.143958Z","iopub.status.idle":"2022-05-23T17:02:22.152245Z","shell.execute_reply.started":"2022-05-23T17:02:22.143920Z","shell.execute_reply":"2022-05-23T17:02:22.151412Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":"# Load our data\n\nAnd have a look on it:","metadata":{}},{"cell_type":"code","source":"#load data\ntrain_data=pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:02:22.153869Z","iopub.execute_input":"2022-05-23T17:02:22.154409Z","iopub.status.idle":"2022-05-23T17:02:22.187843Z","shell.execute_reply.started":"2022-05-23T17:02:22.154368Z","shell.execute_reply":"2022-05-23T17:02:22.187110Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"test_data=pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ntest_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:02:22.189100Z","iopub.execute_input":"2022-05-23T17:02:22.189356Z","iopub.status.idle":"2022-05-23T17:02:22.210580Z","shell.execute_reply.started":"2022-05-23T17:02:22.189321Z","shell.execute_reply":"2022-05-23T17:02:22.209853Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"markdown","source":"# See if data is balanced\n\nAnd as we can see from the histogram - our data is completely balanced.","metadata":{}},{"cell_type":"code","source":"sns.countplot(train_data['target'])\nplt.title('Not about disaster:'+str(train_data.target.value_counts()[0])+'\\n'+\n         'Real disaster:'+str(train_data.target.value_counts()[1]))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:02:22.212959Z","iopub.execute_input":"2022-05-23T17:02:22.213358Z","iopub.status.idle":"2022-05-23T17:02:22.362351Z","shell.execute_reply.started":"2022-05-23T17:02:22.213321Z","shell.execute_reply":"2022-05-23T17:02:22.361569Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"markdown","source":"**Also we must calculate how long words in our dataset.**","metadata":{}},{"cell_type":"code","source":"def words_len(arr, text):\n    word_len = []\n    for i in arr:\n        word_len.append(len(i.split(' ')))\n\n    plt.figure(figsize=(12,6))\n    sns.countplot(word_len)\n    plt.xlabel(\"Lengths of words:\")\n    plt.ylabel('Len repeats:')\n    plt.title(text)\n    plt.show()\n    \nwords_len(train_data['text'],\"Train data_set\")","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:02:22.363877Z","iopub.execute_input":"2022-05-23T17:02:22.366624Z","iopub.status.idle":"2022-05-23T17:02:22.730493Z","shell.execute_reply.started":"2022-05-23T17:02:22.366573Z","shell.execute_reply":"2022-05-23T17:02:22.729698Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"markdown","source":"# Clear data set\nAs we see in the start - data contains a lot of Null in keyword and location fields.\n\nFor future prediction we dont need that colowns. ","metadata":{}},{"cell_type":"code","source":"train_data.drop(['keyword','location'], axis=1, inplace=True)\ntest_data.drop(['keyword','location'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:02:22.731822Z","iopub.execute_input":"2022-05-23T17:02:22.732275Z","iopub.status.idle":"2022-05-23T17:02:22.739699Z","shell.execute_reply.started":"2022-05-23T17:02:22.732237Z","shell.execute_reply":"2022-05-23T17:02:22.738893Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"markdown","source":"**Now time to find most popular words in dataset:**","metadata":{}},{"cell_type":"code","source":"def show_words(arr):\n    tmp = ''\n    for i in arr:\n        tmp +=i\n    \n    wc = WordCloud(collocations = False, background_color = 'white').generate(tmp)\n    plt.figure(figsize=(10,10))\n    plt.imshow(wc, interpolation='bilinear')\n\n    plt.axis(\"off\")\n\n    plt.show()\n\nshow_words(train_data['text'])","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:02:22.741302Z","iopub.execute_input":"2022-05-23T17:02:22.741699Z","iopub.status.idle":"2022-05-23T17:02:23.345582Z","shell.execute_reply.started":"2022-05-23T17:02:22.741662Z","shell.execute_reply":"2022-05-23T17:02:23.343864Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"markdown","source":"As we see - some of this words a meanless. Like \"a\", \"co\",\"u\",\"WH370\" etc.\n\nSo next step is clean our data. Delete all stopwords, words with len < 4, brecets and so on.\n\nAlso convert all text to lowerCase and delete all non English letters and number.","metadata":{}},{"cell_type":"code","source":"swords=set(stopwords.words('english'))\n\ndef clear_txt(text):\n    h_str = text.lower()\n    h_str = re.sub(r'(http|https)?\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b','',h_str)\n    \n    h_str = re.sub(r'\\{[^)]*\\}', '', h_str)\n    h_str = re.sub(r'\\([^)]*\\)', '', h_str)\n    \n    h_str = re.sub('[^a-zA-Z]', ' ', h_str)\n    \n    tokens = [w for w in h_str.split() if not w in swords] \n    \n    res = []\n    for i in tokens:\n        if len(i) >=4:\n            res.append(i)\n    return (\" \".join(res)).strip()","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:02:23.346855Z","iopub.execute_input":"2022-05-23T17:02:23.349365Z","iopub.status.idle":"2022-05-23T17:02:23.357756Z","shell.execute_reply.started":"2022-05-23T17:02:23.349322Z","shell.execute_reply":"2022-05-23T17:02:23.356808Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"clean_train = []\nfor i in train_data['text']:\n    clean_train.append(clear_txt(i))","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:02:23.359096Z","iopub.execute_input":"2022-05-23T17:02:23.359447Z","iopub.status.idle":"2022-05-23T17:02:23.598310Z","shell.execute_reply.started":"2022-05-23T17:02:23.359409Z","shell.execute_reply":"2022-05-23T17:02:23.597564Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"markdown","source":"**Lets look on some example of our work**","metadata":{}},{"cell_type":"code","source":"print(\"Before: \", train_data['text'][7])\nprint(\"After: \", clean_train[7])","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:02:23.599562Z","iopub.execute_input":"2022-05-23T17:02:23.599811Z","iopub.status.idle":"2022-05-23T17:02:23.608281Z","shell.execute_reply.started":"2022-05-23T17:02:23.599778Z","shell.execute_reply":"2022-05-23T17:02:23.607546Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"markdown","source":"**And wordsCloud also become much clear and meaningful**","metadata":{}},{"cell_type":"code","source":"show_words(clean_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:02:23.610120Z","iopub.execute_input":"2022-05-23T17:02:23.610688Z","iopub.status.idle":"2022-05-23T17:02:24.138712Z","shell.execute_reply.started":"2022-05-23T17:02:23.610650Z","shell.execute_reply":"2022-05-23T17:02:24.138052Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"clean_test = []\nfor i in test_data['text']:\n    clean_test.append(clear_txt(i))","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:02:24.139899Z","iopub.execute_input":"2022-05-23T17:02:24.140530Z","iopub.status.idle":"2022-05-23T17:02:24.294663Z","shell.execute_reply.started":"2022-05-23T17:02:24.140457Z","shell.execute_reply":"2022-05-23T17:02:24.293871Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"markdown","source":"# Final result\n\nIn data we left only important to us words. And and their len distribute like this:","metadata":{}},{"cell_type":"code","source":"words_len(clean_train,\"Train data_set\")\nwords_len(clean_test,\"Test data_set\")","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:02:24.298172Z","iopub.execute_input":"2022-05-23T17:02:24.298808Z","iopub.status.idle":"2022-05-23T17:02:25.070675Z","shell.execute_reply.started":"2022-05-23T17:02:24.298768Z","shell.execute_reply":"2022-05-23T17:02:25.069559Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"markdown","source":"# Prepare data for training\n\nDivade data into train and validation set (4:1).","metadata":{}},{"cell_type":"code","source":"#models\nX_train,X_valid,y_train,y_valid = train_test_split(clean_train, train_data['target'], test_size = 0.2, random_state = 40)\nprint(f\"Train size: {len(X_train)}, {len(y_train)}\")\nprint(f\"Validation size: {len(X_valid)}, {len(y_valid)}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:02:25.075466Z","iopub.execute_input":"2022-05-23T17:02:25.076698Z","iopub.status.idle":"2022-05-23T17:02:25.096266Z","shell.execute_reply.started":"2022-05-23T17:02:25.076645Z","shell.execute_reply":"2022-05-23T17:02:25.095048Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"markdown","source":"# Next let create vocabluart by tokinaize our tweets.\n**As we can see from previous gistograms - max len of words is 20.**","metadata":{}},{"cell_type":"code","source":"#as we see from plot:\nmax_len = 20\n\ntokenizer=Tokenizer(oov_token='<OOV>')\ntokenizer.fit_on_texts(X_train)\n\nX_train=tokenizer.texts_to_sequences(X_train)\nX_valid=tokenizer.texts_to_sequences(X_valid)\n\nX_test=tokenizer.texts_to_sequences(clean_test)\n\nX_train=pad_sequences(X_train,maxlen=max_len,padding='post')\nX_valid=pad_sequences(X_valid,maxlen=max_len,padding='post')\n\nX_test=pad_sequences(X_test,maxlen=max_len,padding='post')\n\nvoc = len(tokenizer.word_index) + 1\nprint(\"Vocabluary size is \", voc)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:02:25.101333Z","iopub.execute_input":"2022-05-23T17:02:25.104341Z","iopub.status.idle":"2022-05-23T17:02:26.641881Z","shell.execute_reply.started":"2022-05-23T17:02:25.104301Z","shell.execute_reply":"2022-05-23T17:02:26.641153Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"markdown","source":"**Next we must categorize our data (about disaster and fake)**","metadata":{}},{"cell_type":"code","source":"y_train=to_categorical(y_train,num_classes=2)\ny_valid=to_categorical(y_valid,num_classes=2)\n\nprint(y_train.shape)\nprint(y_valid.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:02:26.643379Z","iopub.execute_input":"2022-05-23T17:02:26.643820Z","iopub.status.idle":"2022-05-23T17:02:26.649804Z","shell.execute_reply.started":"2022-05-23T17:02:26.643784Z","shell.execute_reply":"2022-05-23T17:02:26.649121Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"markdown","source":"# First model\n\nWe build first model - with 1 LSTM layer. \n\nTo prevent the perpetual increase in loss, we use the relu activation function.\nAnd output is sigmoid becouse we have binar clasifications. \n**Same in next models too.**","metadata":{}},{"cell_type":"code","source":"K.clear_session()\n\nmodel1 = Sequential()\nmodel1.add(Embedding(voc,100,input_length=max_len,trainable=True,mask_zero=True))\nmodel1.add(LSTM(300,dropout=0.1,recurrent_dropout=0.2))\nmodel1.add(Dense(64,activation='relu'))\nmodel1.add(Dense(2,activation='sigmoid'))\nmodel1.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:02:26.651128Z","iopub.execute_input":"2022-05-23T17:02:26.651673Z","iopub.status.idle":"2022-05-23T17:02:26.931554Z","shell.execute_reply.started":"2022-05-23T17:02:26.651636Z","shell.execute_reply":"2022-05-23T17:02:26.928183Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"markdown","source":"**Compile our model, using most popular and simple optimazer for RNN - ADAM.**\n\nAnd also divade data into batches with size 650. If this value is lower, then the growth of loss is too rapid. You can put more, but it is important not to cross the line when 30 epochs are not enough for full-fledged learning.","metadata":{}},{"cell_type":"code","source":"model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n\nh1 = model1.fit(x=np.array(X_train), y=np.array(y_train),batch_size = 650, epochs=30,\n                   validation_data=(np.array(X_valid),np.array(y_valid)))","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:02:26.932952Z","iopub.execute_input":"2022-05-23T17:02:26.933227Z","iopub.status.idle":"2022-05-23T17:03:10.332995Z","shell.execute_reply.started":"2022-05-23T17:02:26.933175Z","shell.execute_reply":"2022-05-23T17:03:10.332254Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"markdown","source":"# 1st result\nOn graphics we see that validation loss high, but it's normal (lasts are 1.1 and 1.97).\n\nAnd accuracy in between 0.75 and 0.8 (last is +-0.76).","metadata":{}},{"cell_type":"code","source":"plt.plot(h1.history['val_loss'],'r',label='val_loss')\nplt.plot(h1.history['loss'],'g',label='train_loss')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:03:10.335251Z","iopub.execute_input":"2022-05-23T17:03:10.335540Z","iopub.status.idle":"2022-05-23T17:03:10.531181Z","shell.execute_reply.started":"2022-05-23T17:03:10.335494Z","shell.execute_reply":"2022-05-23T17:03:10.530430Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"plt.plot(h1.history['val_acc'],'b',label='val_acc')\nplt.plot(h1.history['acc'],'y',label='train_acc')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:03:10.532314Z","iopub.execute_input":"2022-05-23T17:03:10.533139Z","iopub.status.idle":"2022-05-23T17:03:10.725666Z","shell.execute_reply.started":"2022-05-23T17:03:10.533101Z","shell.execute_reply":"2022-05-23T17:03:10.725016Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"markdown","source":"# Model 2\n**We build second model - with 1 GRU layer.**\n\nHyper parameters is same as before.","metadata":{}},{"cell_type":"code","source":"model2=Sequential()\nmodel2.add(Embedding(voc,100,input_length=max_len,trainable=True,mask_zero=True))\nmodel2.add(GRU(300,dropout=0.1,recurrent_dropout=0.2))\nmodel2.add(Dense(64,activation='relu'))\nmodel2.add(Dense(2,activation='sigmoid'))\nmodel2.summary()\n\nmodel2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:03:10.726944Z","iopub.execute_input":"2022-05-23T17:03:10.727191Z","iopub.status.idle":"2022-05-23T17:03:10.898663Z","shell.execute_reply.started":"2022-05-23T17:03:10.727157Z","shell.execute_reply":"2022-05-23T17:03:10.897883Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"h2=model2.fit(x=np.array(X_train),y=np.array(y_train),batch_size=650,epochs=30,\n          validation_data=(np.array(X_valid),np.array(y_valid)))","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:03:10.899976Z","iopub.execute_input":"2022-05-23T17:03:10.900251Z","iopub.status.idle":"2022-05-23T17:03:43.046298Z","shell.execute_reply.started":"2022-05-23T17:03:10.900216Z","shell.execute_reply":"2022-05-23T17:03:43.045601Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"markdown","source":"# Second result\nOn graphics we see that validation loss high, but it's smaller than 1st model (last is 1.3).\n\nAnd accuracy in between 0.75 and 0.8 (last is +-0.77).\n","metadata":{}},{"cell_type":"code","source":"plt.plot(h2.history['val_loss'],'r',label='val_loss')\nplt.plot(h2.history['loss'],'g',label='train_loss')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:03:43.048025Z","iopub.execute_input":"2022-05-23T17:03:43.048290Z","iopub.status.idle":"2022-05-23T17:03:43.238186Z","shell.execute_reply.started":"2022-05-23T17:03:43.048254Z","shell.execute_reply":"2022-05-23T17:03:43.237456Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"plt.plot(h2.history['val_acc'],'b',label='val_acc')\nplt.plot(h2.history['acc'],'y',label='train_acc')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:03:43.239513Z","iopub.execute_input":"2022-05-23T17:03:43.239922Z","iopub.status.idle":"2022-05-23T17:03:43.427912Z","shell.execute_reply.started":"2022-05-23T17:03:43.239883Z","shell.execute_reply":"2022-05-23T17:03:43.427228Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"markdown","source":"# 3rd model\n**Lets try multylayer LSTM.** Because it have better result (the difference is small, but it is there)\n\nLets add 4 LSTM layer with size 350,150,50.\nButch size now 200. We have greater losses, but the quality of education must increase.\n\nOther hyper parametrs are same.","metadata":{}},{"cell_type":"code","source":"# i think best result is LSTM, so lent add another layer\nmodel3=Sequential()\nmodel3.add(Embedding(voc,100,input_length=max_len,trainable=True,mask_zero=True))\nmodel3.add(LSTM(350,dropout=0.1,recurrent_dropout=0.2,return_sequences=True))\nmodel3.add(LSTM(150,dropout=0.1,recurrent_dropout=0.2,return_sequences=True))\nmodel3.add(LSTM(50,dropout=0.1,recurrent_dropout=0.2))\nmodel3.add(Dense(64,activation='relu'))\nmodel3.add(Dense(2,activation='sigmoid'))\nmodel3.summary()\n\nmodel3.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:06:42.619921Z","iopub.execute_input":"2022-05-23T17:06:42.620250Z","iopub.status.idle":"2022-05-23T17:06:43.065737Z","shell.execute_reply.started":"2022-05-23T17:06:42.620213Z","shell.execute_reply":"2022-05-23T17:06:43.065046Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"h3=model3.fit(x=np.array(X_train),y=np.array(y_train),batch_size=200,epochs=30,\n          validation_data=(np.array(X_valid),np.array(y_valid)))","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:06:49.098725Z","iopub.execute_input":"2022-05-23T17:06:49.098976Z","iopub.status.idle":"2022-05-23T17:11:41.913086Z","shell.execute_reply.started":"2022-05-23T17:06:49.098946Z","shell.execute_reply":"2022-05-23T17:11:41.912389Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"markdown","source":"# 3rd result\nOn graphics we see that validation loss higher (last is 1.4).\n\nAnd accuracy in between 0.74 and 0.8 (last is +-0.77).","metadata":{}},{"cell_type":"code","source":"plt.plot(h3.history['val_loss'],'r',label='val_loss')\nplt.plot(h3.history['loss'],'g',label='train_loss')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:13:01.220092Z","iopub.execute_input":"2022-05-23T17:13:01.220815Z","iopub.status.idle":"2022-05-23T17:13:01.410697Z","shell.execute_reply.started":"2022-05-23T17:13:01.220775Z","shell.execute_reply":"2022-05-23T17:13:01.410037Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"plt.plot(h3.history['val_acc'],'b',label='val_acc')\nplt.plot(h3.history['acc'],'y',label='train_acc')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:13:04.979670Z","iopub.execute_input":"2022-05-23T17:13:04.979928Z","iopub.status.idle":"2022-05-23T17:13:05.166637Z","shell.execute_reply.started":"2022-05-23T17:13:04.979898Z","shell.execute_reply":"2022-05-23T17:13:05.165985Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"markdown","source":"# Last model\n**Lets try multylayer GRU.** Just for compare it with previous. \n\nLets add 4 GRU layer with size 350,150,50.\nButch size now 200. We have greater losses, but the quality of education must increase.\n\nOther hyper parametrs are same.","metadata":{}},{"cell_type":"code","source":"model4=Sequential()\nmodel4.add(Embedding(voc,100,input_length=max_len,trainable=True,mask_zero=True))\nmodel4.add(GRU(350,dropout=0.1,recurrent_dropout=0.2,return_sequences=True))\nmodel4.add(GRU(150,dropout=0.1,recurrent_dropout=0.2,return_sequences=True))\nmodel4.add(GRU(50,dropout=0.1,recurrent_dropout=0.2))\nmodel4.add(Dense(64,activation='relu'))\nmodel4.add(Dense(2,activation='sigmoid'))\nmodel4.summary()\n\nmodel4.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:13:10.620657Z","iopub.execute_input":"2022-05-23T17:13:10.620911Z","iopub.status.idle":"2022-05-23T17:13:11.030807Z","shell.execute_reply.started":"2022-05-23T17:13:10.620881Z","shell.execute_reply":"2022-05-23T17:13:11.030117Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"h4=model4.fit(x=np.array(X_train),y=np.array(y_train),batch_size=200,epochs=30,\n          validation_data=(np.array(X_valid),np.array(y_valid)))","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:13:16.153700Z","iopub.execute_input":"2022-05-23T17:13:16.154303Z","iopub.status.idle":"2022-05-23T17:18:24.518319Z","shell.execute_reply.started":"2022-05-23T17:13:16.154265Z","shell.execute_reply":"2022-05-23T17:18:24.517305Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"markdown","source":"# 4th result\nOn graphics we see that validation loss higher (last is +-1.5).\n\nAnd accuracy in between 0.73 and 0.8 (last is +-0.74).","metadata":{}},{"cell_type":"code","source":"plt.plot(h4.history['val_loss'],'r',label='val_loss')\nplt.plot(h4.history['loss'],'g',label='train_loss')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:19:03.422086Z","iopub.execute_input":"2022-05-23T17:19:03.422797Z","iopub.status.idle":"2022-05-23T17:19:03.612628Z","shell.execute_reply.started":"2022-05-23T17:19:03.422757Z","shell.execute_reply":"2022-05-23T17:19:03.611917Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"plt.plot(h4.history['val_acc'],'b',label='val_acc')\nplt.plot(h4.history['acc'],'y',label='train_acc')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:19:06.727511Z","iopub.execute_input":"2022-05-23T17:19:06.728034Z","iopub.status.idle":"2022-05-23T17:19:06.915671Z","shell.execute_reply.started":"2022-05-23T17:19:06.727998Z","shell.execute_reply":"2022-05-23T17:19:06.914992Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"markdown","source":"# Final submition\n\n**So, the best model is model 3...**\n> (But from start to start we can see any modes can win)","metadata":{}},{"cell_type":"code","source":"predict_help = model3.predict(X_test)\nprediction = [0 if i[0]>=0.5 else 1 for i in predict_help]\ntest_data['target'] = prediction\n\nsub = test_data[['id', 'target']]\nprint(\"Start write into csv\")\nsub.to_csv('Submission.csv', index=False)\nprint(\"End write into csv\")","metadata":{"execution":{"iopub.status.busy":"2022-05-23T17:21:06.840174Z","iopub.execute_input":"2022-05-23T17:21:06.840746Z","iopub.status.idle":"2022-05-23T17:21:09.497923Z","shell.execute_reply.started":"2022-05-23T17:21:06.840711Z","shell.execute_reply":"2022-05-23T17:21:09.497180Z"},"trusted":true},"execution_count":133,"outputs":[]}]}